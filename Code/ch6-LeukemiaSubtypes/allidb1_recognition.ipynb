{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful anal,ytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import allidb1_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL_IDB1/resized_im\n",
      "ALL_IDB1/im\n",
      "[[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "# Resize img folder (size divided by 10)\n",
    "#preprocess.resize_folder(\"Datasets/ALL_IDB1/im\")\n",
    "\n",
    "# Create CSV from img folder\n",
    "myFileList = allidb1_preproc.createFileList('ALL_IDB1/resized_im') \n",
    "#allidb1_preproc.data_to_CSV(myFileList)\n",
    "\n",
    "# get Y labels from the data \n",
    "myFileListForY = allidb1_preproc.createFileList('ALL_IDB1/im') \n",
    "y = allidb1_preproc.get_yLabels(myFileListForY)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and test data files\n",
    "# ATTENTION: POUR L'INSTANT JE DOIS MODIFIER LA PREMIERE LIGNE MANUELLEMENT (A FAIRE AUTOMATIQUEMENt)\n",
    "X_train = pd.read_csv(\"resizedALLIDB1.csv\").values\n",
    "X_test  = pd.read_csv(\"resizedALLIDB1.csv\").values\n",
    "y_train = y\n",
    "y_test = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
    "trainX = X_train.reshape(X_train.shape[0],1,100, 100).astype( 'float32' )\n",
    "X_train = trainX / 255.0 \n",
    "\n",
    "testX = X_test.reshape(X_test.shape[0],1,100, 100).astype( 'float32' )\n",
    "X_test = testX / 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of classes that are in the dataset, so we know how many neurons to compress the final layer down to \n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "class_num = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "K.set_image_dim_ordering('th')\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=(1, 100, 100), activation= 'relu' ))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(15, (3, 3), activation= 'relu' ))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation= 'relu' ))\n",
    "model.add(Dense(50, activation= 'relu' ))\n",
    "model.add(Dense(class_num, activation= 'softmax' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Compile model\n",
    "model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 14ms/step - loss: 0.6933 - acc: 0.4815\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6986 - acc: 0.5463\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6931 - acc: 0.4537\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6834 - acc: 0.5278\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6673 - acc: 0.7407\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6549 - acc: 0.7407\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6385 - acc: 0.7500\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6196 - acc: 0.8148\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6010 - acc: 0.7037\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.5785 - acc: 0.8056\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.5359 - acc: 0.8241\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.5153 - acc: 0.8426\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.4942 - acc: 0.8704\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.4721 - acc: 0.7870\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.4731 - acc: 0.7685\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.4404 - acc: 0.7685\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.3987 - acc: 0.8241\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.4317 - acc: 0.8056\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.3600 - acc: 0.8519\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.3679 - acc: 0.8148\n",
      "108/108 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,epochs=20,batch_size=160)\n",
    "score = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
